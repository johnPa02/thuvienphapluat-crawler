# Batch Crawler for Thuvienphapluat.vn

Multi-threaded batch crawler sá»­ dá»¥ng **pipeline.py** Ä‘á»ƒ xá»­ lÃ½ nhiá»u URL cÃ¹ng lÃºc vá»›i tÃ­nh nÄƒng quáº£n lÃ½ hÃ ng Ä‘á»£i, theo dÃµi tiáº¿n Ä‘á»™ vÃ  phá»¥c há»“i khi lá»—i.

## TÃ­nh nÄƒng

- ğŸš€ **Multi-threaded**: Xá»­ lÃ½ nhiá»u URL Ä‘á»“ng thá»i vá»›i sá»‘ lÆ°á»£ng thread tÃ¹y chá»‰nh
- ğŸ“Š **Progress Tracking**: Hiá»ƒn thá»‹ tiáº¿n Ä‘á»™ real-time vá»›i thá»‘ng kÃª chi tiáº¿t
- ğŸ”„ **Resume Functionality**: Tiáº¿p tá»¥c tá»« vá»‹ trÃ­ Ä‘Ã£ dá»«ng trÆ°á»›c Ä‘Ã³
- ğŸ›¡ï¸ **Error Handling**: Xá»­ lÃ½ lá»—i vá»›i cÆ¡ cháº¿ retry thÃ´ng minh
- â±ï¸ **Rate Limiting**: Tá»± Ä‘á»™ng delay giá»¯a cÃ¡c request Ä‘á»ƒ trÃ¡nh bá»‹ block
- ğŸ’¾ **State Management**: LÆ°u tráº¡ng thÃ¡i Ä‘á»ƒ phá»¥c há»“i khi cáº§n
- ğŸ“ **Logging**: Ghi láº¡i URL thÃ nh cÃ´ng vÃ  tháº¥t báº¡i
- ğŸ”— **Pipeline Integration**: Gá»i trá»±c tiáº¿p `uv run python pipeline.py` Ä‘á»ƒ Ä‘áº£m báº£o Ä‘á»“ng bá»™ hoÃ n toÃ n
- ğŸ“ **Custom Output Directory**: Tá»• chá»©c files vÃ o thÆ° má»¥c riÃªng Ä‘á»ƒ dá»… quáº£n lÃ½

## Kiáº¿n trÃºc

Batch crawler nÃ y **KHÃ”NG** implement láº¡i logic crawl mÃ  **gá»i trá»±c tiáº¿p** `pipeline.py` thÃ´ng qua subprocess:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   URL Queue     â”‚â”€â”€â”€â–¶â”‚  Worker Thread 1 â”‚â”€â”€â”€â–¶â”‚  uv run python  â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚  pipeline.py    â”‚
â”‚                 â”‚â”€â”€â”€â–¶â”‚  Worker Thread 2 â”‚â”€â”€â”€â–¶â”‚                 â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚                 â”‚â”€â”€â”€â–¶â”‚  Worker Thread N â”‚â”€â”€â”€â–¶â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚  Output Dir/     â”‚
                          â”‚  crawl/          â”‚
                          â”‚  Nghá»‹_Ä‘á»‹nh*.txt  â”‚
                          â”‚  failed_urls.txtâ”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Lá»£i Ã­ch:**
- âœ… **Äá»“ng bá»™ 100%** vá»›i pipeline.py Ä‘Ã£ tá»‘i Æ°u
- âœ… **KhÃ´ng duplicate code**
- âœ… **Dá»… maintain** khi update pipeline
- âœ… **DÃ¹ng láº¡i toÃ n bá»™ features** cá»§a pipeline
- âœ… **Tá»• chá»©c files gá»n gÃ ng** trong thÆ° má»¥c riÃªng

## CÃ i Ä‘áº·t

```bash
# Install dependencies (cáº§n cho pipeline.py)
pip install playwright beautifulsoup4

# Install Playwright browsers
playwright install chromium

# Hoáº·c dÃ¹ng uv (recommended)
pip install uv
uv sync
```

## Sá»­ dá»¥ng

### CÆ¡ báº£n

```bash
python batch_crawler.py example_urls.txt
```

### NÃ¢ng cao

```bash
# Sá»­ dá»¥ng 8 threads vÃ  custom output directory
python batch_crawler.py example_urls.txt --threads 8 --output-dir crawled_docs

# TÃ¹y chá»‰nh cookie file
python batch_crawler.py example_urls.txt --cookies cookies.txt --output-dir legal_docs
source .venv/bin/activate
# TÃ¹y chá»‰nh delay giá»¯a cÃ¡c requests (2-5 giÃ¢y)
python batch_crawler.py example_urls.txt --delay 2 5 --output-dir temp_crawl

# TÃ¹y chá»‰nh sá»‘ láº§n retry
python batch_crawler.py example_urls.txt --retry 5 --output-dir retry_crawl

# Tiáº¿p tá»¥c tá»« láº§n cháº¡y trÆ°á»›c
python batch_crawler.py example_urls.txt --resume --output-dir continued_crawl
```

### ToÃ n bá»™ tham sá»‘

```bash
python batch_crawler.py <url_file> [options]

Arguments:
  url_file              File chá»©a danh sÃ¡ch URLs

Options:
  -t, --threads N       Sá»‘ thread concurrently (default: 4)
  -c, --cookies FILE    Cookie file (default: cookies.txt)
  -d, --delay MIN MAX   Delay range giá»¯a requests (default: 1.0 3.0)
  -r, --retry N         Sá»‘ láº§n retry cho má»—i URL (default: 3)
  --resume              Tiáº¿p tá»¥c tá»« láº§n cháº¡y trÆ°á»›c
  --state FILE          File state cho resume (default: crawl_state.json)
  -o, --output-dir DIR  ThÆ° má»¥c output cho files Ä‘Ã£ crawl (default: crawl)
```

## Äá»‹nh dáº¡ng URL file

Táº¡o file text vá»›i má»—i URL trÃªn má»™t dÃ²ng:

```
# example_urls.txt
https://thuvienphapluat.vn/van-ban/Doanh-nghiep/Nghi-dinh-47-2021-ND-CP-...
https://thuvienphapluat.vn/van-ban/Phap-luat/Doanh-nghiep-59-2020-QH14-...
# DÃ²ng báº¯t Ä‘áº§u báº±ng # sáº½ bá»‹ bá» qua
```

## Output Structure

Khi cháº¡y vá»›i `--output-dir crawl`, cáº¥u trÃºc thÆ° má»¥c sáº½ lÃ :

```
project/
â”œâ”€â”€ batch_crawler.py
â”œâ”€â”€ pipeline.py
â”œâ”€â”€ cookies.txt
â”œâ”€â”€ example_urls.txt
â”œâ”€â”€ crawl_state.json           # State file (luu á»Ÿ root)
â”œâ”€â”€ crawl/                     # Output directory
â”‚   â”œâ”€â”€ Nghá»‹_Ä‘á»‹nh_47-2021-NÄ-CP.txt
â”‚   â”œâ”€â”€ Luáº­t_Doanh_nghiá»‡p_2020.txt
â”‚   â”œâ”€â”€ Nghá»‹_Ä‘á»‹nh_248-2025-NÄ-CP.txt
â”‚   â””â”€â”€ failed_urls.txt        # URLs tháº¥t báº¡i
â””â”€â”€ crawled_docs/              # Directory khÃ¡c khi custom
    â”œâ”€â”€ ...
    â””â”€â”€ ...
```

## Pipeline Integration

Batch crawler sá»­ dá»¥ng command sau Ä‘á»ƒ gá»i pipeline.py:

```bash
# Change Ä‘áº¿n output directory
cd crawl/

# Run pipeline vá»›i full path cookies
uv run python pipeline.py <URL> --cookies ../cookies.txt --doc-name "<tÃªn_vÄƒn_báº£n>"

# Pipeline sáº½ save file trong directory hiá»‡n táº¡i
# -> crawl/Nghá»‹_Ä‘á»‹nh_47-2021-NÄ-CP.txt
```

### Directory Management
- **Auto-create**: ThÆ° má»¥c output Ä‘Æ°á»£c táº¡o tá»± Ä‘á»™ng náº¿u chÆ°a tá»“n táº¡i
- **Directory switching**: Pipeline cháº¡y trong output directory
- **Cookie path**: Cookies path Ä‘Æ°á»£c resolved tá»« original directory
- **State file**: State file váº«n lÆ°u á»Ÿ root directory

## Tá»‘i Æ°u hiá»‡u suáº¥t

### TÃ¹y chá»‰nh sá»‘ threads
- **1-4 threads**: An toÃ n, Ã­t kháº£ nÄƒng bá»‹ block
- **4-8 threads**: CÃ¢n báº±ng giá»¯a tá»‘c Ä‘á»™ vÃ  stability
- **8+ threads**: Nhanh nháº¥t nhÆ°ng cÃ³ thá»ƒ bá»‹ block

### TÃ¹y chá»‰nh delay
- **1-3 giÃ¢y**: Máº·c Ä‘á»‹nh, phÃ¹ há»£p cho háº§u háº¿t cases
- **3-5 giÃ¢y**: An toÃ n hÆ¡n cho server nháº¡y cáº£m
- **0.5-2 giÃ¢y**: Nhanh hÆ¡n nhÆ°ng tÄƒng rá»§i ro

### Output Directory Strategy
- **Theo loáº¡i vÄƒn báº£n**: `crawl/ Nghá»‹_Ä‘á»‹nh/`, `crawl/Luáº­t/`, `crawl/ThÃ´ng_tÆ°/`
- **Theo ngÃ y**: `crawl/2025-01-15/`, `crawl/2025-01-16/`
- **Theo dá»± Ã¡n**: `crawl/doanh_nghiep/`, `crawl/lao_dong/`

### Máº¹o sá»­ dá»¥ng
1. **Báº¯t Ä‘áº§u vá»›i sá»‘ thread Ã­t**: Test vá»›i 2-4 threads trÆ°á»›c
2. **Sá»­ dá»¥ng directory cÃ³ Ã½ nghÄ©a**: `--output-dir nghá»‹_Ä‘á»‹nh_2025`
3. **Giáº£m delay náº¿u cáº§n tá»‘c Ä‘á»™**: TÄƒng dáº§n lÃªn khi cáº§n
4. **Sá»­ dá»¥ng resume**: LuÃ´n dÃ¹ng `--resume` Ä‘á»ƒ khÃ´ng pháº£i crawl láº¡i
5. **Monitor failed URLs**: Kiá»ƒm tra file `failed_urls.txt` trong output directory
6. **Kiá»ƒm tra pipeline.py**: Äáº£m báº£o pipeline.py hoáº¡t Ä‘á»™ng trÆ°á»›c khi cháº¡y batch

## VÃ­ dá»¥ thá»±c táº¿

```bash
# Crawl Nghá»‹ Ä‘á»‹nh vÃ o thÆ° má»¥c riÃªng
python batch_crawler.py nghi_dinh_urls.txt --output-dir crawl/nghi_dinh --threads 6

# Crawl nhiá»u loáº¡i vÄƒn báº£n
python batch_crawler.py all_urls.txt --output-dir crawl/2025-01-17 --threads 8

# Crawl vá»›i cookies Ä‘á»ƒ trÃ¡nh login
python batch_crawler.py urls.txt --cookies cookies.txt --output-dir authenticated_crawl --threads 4

# Resume sau khi bá»‹ lá»—i
python batch_crawler.py urls.txt --resume --output-dir continued_crawl --threads 3

# Crawl nhanh (risk more)
python batch_crawler.py urls.txt --threads 8 --delay 0.5 1.5 --output-dir fast_crawl
```

## Troubleshooting

### Common Issues

1. **"Pipeline tháº¥t báº¡i"**
   - Kiá»ƒm tra pipeline.py hoáº¡t Ä‘á»™ng: `uv run python pipeline.py "URL"`
   - Kiá»ƒm tra dependencies: `uv sync`
   - Xem stderr output Ä‘á»ƒ chi tiáº¿t lá»—i

2. **"Pipeline timeout sau 5 phÃºt"**
   - URL quÃ¡ cháº­m hoáº·c server response cháº­m
   - TÄƒng timeout trong code náº¿u cáº§n

3. **"KhÃ´ng tÃ¬m tháº¥y file output"**
   - Pipeline output format Ä‘Ã£ thay Ä‘á»•i
   - Kiá»ƒm tra manual output cá»§a pipeline.py

4. **"Permission denied" khi táº¡o directory**
   - Kiá»ƒm tra permissions cá»§a thÆ° má»¥c cha
   - Cháº¡y vá»›i appropriate user permissions

5. **Files bá»‹ lÆ°u sai directory**
   - Kiá»ƒm tra output directory path
   - Xem log output Ä‘á»ƒ confirm directory Ä‘Ã£ Ä‘Æ°á»£c táº¡o

### Debug mode

Äá»ƒ debug, cháº¡y pipeline.py manual trÆ°á»›c:

```bash
# Test má»™t URL trong output directory
cd crawl
uv run python ../pipeline.py "https://thuvienphapluat.vn/van-ban/..." --cookies ../cookies.txt

# Kiá»ƒm tra output directory
ls -la crawl/
```

## Performance Tips

1. **Start small**: Test vá»›i 5-10 URLs trÆ°á»›c
2. **Monitor resources**: CPU vÃ  Memory usage
3. **Adjust based on server response**: TÄƒng/giáº£m thread vÃ  delay
4. **Use resume**: KhÃ´ng báº¯t Ä‘áº§u láº¡i tá»« Ä‘áº§u
5. **Batch processing**: Chia lá»›n URLs thÃ nh cÃ¡c file nhá»
6. **Test pipeline first**: Äáº£m báº£o pipeline.py hoáº¡t Ä‘á»™ng trÆ°á»›c khi cháº¡y batch
7. **Organize by directory**: Sá»­ dá»¥ng output directory cÃ³ Ã½ nghÄ©a

## Advanced Usage

### Organize by Document Type

```bash
# Táº¡o separate directories for different document types
python batch_crawler.py nghi_dinh_urls.txt --output-dir crawl/nghi_dinh
python batch_crawler.py luat_urls.txt --output-dir crawl/luat
python batch_crawler.py thong_tu_urls.txt --output-dir crawl/thong_tu
```

### Organize by Date

```bash
# Crawl with date-based directories
DATE=$(date +%Y-%m-%d)
python batch_crawler.py daily_urls.txt --output-dir "crawl/$DATE"
```

### Clean Old Files

```bash
# Remove old output directory before running
rm -rf crawl/
python batch_crawler.py urls.txt --output-dir crawl

# Or backup and create new
mv crawl/ "backup_$(date +%Y%m%d_%H%M%S)/"
python batch_crawler.py urls.txt --output-dir crawl
```

## Architecture Details

### Subprocess Management
- `subprocess.run()` vá»›i timeout 5 phÃºt
- `os.chdir()` Ä‘á»ƒ change Ä‘áº¿n output directory
- Full path resolution cho cookies file
- Capture stdout/stderr cho debugging

### Directory Management
- `os.makedirs(output_dir, exist_ok=True)` Ä‘á»ƒ táº¡o directory
- Path resolution giá»¯a original vÃ  output directory
- State file váº«n á»Ÿ root Ä‘á»ƒ dá»… dÃ ng resume
- Failed URLs file lÆ°u trong output directory

### Thread Safety
- `threading.Lock()` cho shared data structures
- `queue.Queue()` cho URL management
- Atomic operations cho statistics
- Directory switching thread-safe