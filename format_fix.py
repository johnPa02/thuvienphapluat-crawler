#!/usr/bin/env python3
import argparse
import re
import tiktoken
from pathlib import Path
from typing import List, Tuple


def extract_title(text: str, filename: str) -> str:
    for line in text.splitlines():
        s = line.strip()
        if s:
            return s
    name = Path(filename).stem
    name = name.replace('_', ' ')
    name = re.sub(r'\s+', ' ', name).strip()
    return name


def clean_chunk_lines(chunk: str, title_pattern: str) -> str:
    lines = chunk.splitlines()
    cleaned_lines = []
    noise_re = re.compile(rf'^\s*{title_pattern}\s*\.?\s*$', re.IGNORECASE | re.UNICODE)

    for line in lines:
        if noise_re.fullmatch(line):
            continue
        cleaned_lines.append(line)

    return '\n'.join(cleaned_lines)


def normalize_newlines(text: str) -> str:
    """
    Chu·∫©n h√≥a kho·∫£ng tr·∫Øng th√¥ng minh cho vƒÉn b·∫£n ph√°p lu·∫≠t:
    - Lo·∫°i b·ªè d√≤ng tr·ªëng th·ª´a gi·ªØa c√°c ph·∫ßn c·∫•u tr√∫c (Ch∆∞∆°ng/M·ª•c/ƒêi·ªÅu)
    - Gi·ªØ nguy√™n paragraph break h·ª£p l·ªá b√™n trong n·ªôi dung ƒëi·ªÅu kho·∫£n
    """
    # Chu·∫©n h√≥a 3+ newline th√†nh 2 newline
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    # X·ª≠ l√Ω d√≤ng tr·ªëng th·ª´a SAU c√°c header c·∫•u tr√∫c
    # Pattern 1: Header k·∫øt th√∫c -> 1+ d√≤ng tr·ªëng -> d√≤ng ti·∫øp theo b·∫Øt ƒë·∫ßu b·∫±ng ch·ªØ hoa
    text = re.sub(
        r'(^|\n)(Ch∆∞∆°ng|M·ª•c|ƒêi·ªÅu|PH·ª§ L·ª§C|Bi·ªÉu s·ªë)\s+[^\n]*(?:[.:\n]|$)\s*\n\s*\n\s*([A-Zƒê√Ç√ä√î∆Ø√Ä·∫¢√É·∫†·∫∞·∫≤·∫¥·∫∂·∫¶·∫®·∫™·∫¨√à√â·∫∏·∫∫·∫º·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï·ªí·ªî·ªñ·ªò·ªú·ªû·ª†·ª¢√ô√ö·ª§·ª¶≈®·ª™·ª¨·ªÆ·ª∞·ª≤√ù·ª¥·ª∂·ª∏])',
        r'\1\2 \3',
        text,
        flags=re.IGNORECASE | re.UNICODE | re.MULTILINE
    )
    
    # Pattern 2: "M·ª•c X.\n\nT√äN M·ª§C" -> "M·ª•c X. T√äN M·ª§C"
    text = re.sub(
        r'(M·ª•c\s+[\dIVXLCDM]+\.?)\s*\n\s*\n\s*([A-Zƒê√Ç√ä√î∆Ø√Ä·∫¢√É·∫†·∫∞·∫≤·∫¥·∫∂·∫¶·∫®·∫™·∫¨√à√â·∫∏·∫∫·∫º·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï·ªí·ªî·ªñ·ªò·ªú·ªû·ª†·ª¢√ô√ö·ª§·ª¶≈®·ª™·ª¨·ªÆ·ª∞·ª≤√ù·ª¥·ª∂·ª∏])',
        r'\1 \2',
        text,
        flags=re.IGNORECASE | re.UNICODE
    )
    
    # Pattern 3: Lo·∫°i b·ªè d√≤ng tr·ªëng th·ª´a gi·ªØa "Ch∆∞∆°ng" v√† "M·ª•c/ƒêi·ªÅu" khi ·ªü g·∫ßn nhau
    text = re.sub(
        r'(Ch∆∞∆°ng\s+[IVXLCDM0-9]+[^\n]*)\n\s*\n\s*(M·ª•c|ƒêi·ªÅu)\b',
        r'\1\n\2',
        text,
        flags=re.IGNORECASE | re.UNICODE
    )
    
    # Pattern 4: Lo·∫°i b·ªè d√≤ng tr·ªëng th·ª´a gi·ªØa "M·ª•c" v√† "ƒêi·ªÅu"
    text = re.sub(
        r'(M·ª•c\s+[\dIVXLCDM]+[^\n]*)\n\s*\n\s*(ƒêi·ªÅu)\b',
        r'\1\n\2',
        text,
        flags=re.IGNORECASE | re.UNICODE
    )
    
    return text.strip()


def merge_chuong_muc_chunks(chunks: List[str]) -> List[str]:
    """
    Merge th√¥ng minh c√°c chunk c·∫•u tr√∫c theo hierarchy:
    - Ch∆∞∆°ng ‚Üí (M·ª•c ‚Üí) ƒêi·ªÅu ph·∫£i n·∫±m trong c√πng 1 chunk
    - X·ª≠ l√Ω robust v·ªõi prefix ti√™u ƒë·ªÅ (v√≠ d·ª•: "B·ªô lu·∫≠t d√¢n s·ª± 2015. ")
    - Duy·ªát t·ª´ d∆∞·ªõi l√™n ƒë·ªÉ x·ª≠ l√Ω chain li√™n ti·∫øp
    """
    if not chunks:
        return []
    
    # Patterns linh ho·∫°t - kh√¥ng y√™u c·∫ßu ·ªü ƒë·∫ßu d√≤ng tuy·ªát ƒë·ªëi
    chuong_pattern = re.compile(r'\bCh∆∞∆°ng\s+[IVXLCDM0-9]+\b', re.IGNORECASE | re.UNICODE)
    muc_pattern = re.compile(r'\bM·ª•c\s+[\dIVXLCDM]+\b', re.IGNORECASE | re.UNICODE)
    dieu_pattern = re.compile(r'\bƒêi·ªÅu\s+\d+\w*\s*[.:]', re.IGNORECASE | re.UNICODE)
    phu_luc_pattern = re.compile(r'\bPH·ª§ L·ª§C\s+[IVXLCDM]+\b', re.UNICODE)
    bieu_so_pattern = re.compile(r'\bBi·ªÉu s·ªë\s+\d+\s*:', re.IGNORECASE | re.UNICODE)
    
    def has_terminal_element(text: str) -> bool:
        """Ki·ªÉm tra chunk c√≥ ch·ª©a ƒêi·ªÅu/PH·ª§ L·ª§C/Bi·ªÉu s·ªë kh√¥ng"""
        return (dieu_pattern.search(text) is not None or
                phu_luc_pattern.search(text) is not None or
                bieu_so_pattern.search(text) is not None)
    
    def starts_with_structural(text: str) -> Tuple[bool, bool]:
        """
        Ki·ªÉm tra chunk c√≥ b·∫Øt ƒë·∫ßu b·∫±ng Ch∆∞∆°ng/M·ª•c kh√¥ng (trong 200 k√Ω t·ª± ƒë·∫ßu)
        Tr·∫£ v·ªÅ (is_chuong, is_muc)
        """
        snippet = text[:200]  # Ch·ªâ ki·ªÉm tra ph·∫ßn ƒë·∫ßu ƒë·ªÉ tr√°nh false positive
        
        # Lo·∫°i b·ªè prefix ti√™u ƒë·ªÅ ph·ªï bi·∫øn tr∆∞·ªõc khi ki·ªÉm tra
        cleaned_snippet = re.sub(r'^(B·ªô lu·∫≠t d√¢n s·ª±|Lu·∫≠t|Ngh·ªã ƒë·ªãnh|Th√¥ng t∆∞)\s+\d{4}\.\s*', '', snippet, flags=re.IGNORECASE)
        
        is_chuong = chuong_pattern.search(cleaned_snippet) is not None
        is_muc = muc_pattern.search(cleaned_snippet) is not None
        return is_chuong, is_muc
    
    # B·∫Øt ƒë·∫ßu t·ª´ chunk cu·ªëi c√πng
    result = [chunks[-1].strip()]
    
    # Duy·ªát ng∆∞·ª£c t·ª´ chunk √°p cu·ªëi l√™n ƒë·∫ßu
    for i in range(len(chunks) - 2, -1, -1):
        chunk = chunks[i].strip()
        if not chunk:
            continue
        
        is_chuong, is_muc = starts_with_structural(chunk)
        is_structural = is_chuong or is_muc
        has_terminal = has_terminal_element(chunk)
        
        # N·∫øu l√† Ch∆∞∆°ng/M·ª•c "m·ªì c√¥i" (kh√¥ng ch·ª©a ƒêi·ªÅu/PH·ª§ L·ª§C/Bi·ªÉu s·ªë) ‚Üí merge v√†o chunk ti·∫øp theo
        if is_structural and not has_terminal and result:
            # Merge v·ªõi SINGLE newline ƒë·ªÉ tr√°nh d√≤ng tr·ªëng th·ª´a
            merged_content = chunk + '\n' + result[-1]
            # Chu·∫©n h√≥a ngay sau merge
            merged_content = normalize_newlines(merged_content)
            result[-1] = merged_content
        else:
            result.append(chunk)
    
    # Reverse ƒë·ªÉ tr·∫£ v·ªÅ th·ª© t·ª± ban ƒë·∫ßu
    return result[::-1]


def split_into_chunks(text: str) -> List[str]:
    text = text.replace('\r\n', '\n').replace('\r', '\n')
    matches = []

    # Detect t·∫•t c·∫£ c√°c boundary types
    chuong_re = re.compile(r'\bCh∆∞∆°ng\s+([IVXLCDM0-9]+)\b', re.IGNORECASE | re.UNICODE | re.MULTILINE)
    for m in chuong_re.finditer(text):
        matches.append(m.start())

    muc_re = re.compile(r'\bM·ª•c\s+([\dIVXLCDM]+)\b', re.IGNORECASE | re.UNICODE | re.MULTILINE)
    for m in muc_re.finditer(text):
        matches.append(m.start())

    dieu_re = re.compile(r'\bƒêi·ªÅu\s+(\d+\w*)\s*[.:]', re.IGNORECASE | re.UNICODE | re.MULTILINE)
    for m in dieu_re.finditer(text):
        matches.append(m.start())

    phu_luc_re = re.compile(r'\bPH·ª§ L·ª§C\s+([IVXLCDM]+)\b', re.UNICODE | re.MULTILINE)
    for m in phu_luc_re.finditer(text):
        matches.append(m.start())

    bieu_so_re = re.compile(r'\bBi·ªÉu s·ªë\s+(\d+)\s*:', re.IGNORECASE | re.UNICODE | re.MULTILINE)
    for m in bieu_so_re.finditer(text):
        matches.append(m.start())

    matches = sorted(set(matches))

    if not matches:
        return [text.strip()]

    chunks = []
    if matches[0] > 0:
        preamble = text[:matches[0]].strip()
        if preamble:
            chunks.append(preamble)

    for i in range(len(matches)):
        start = matches[i]
        end = matches[i + 1] if i + 1 < len(matches) else len(text)
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)

    # Merge c√°c chunk c·∫•u tr√∫c theo hierarchy
    chunks = merge_chuong_muc_chunks(chunks)
    
    return chunks


def _get_token_encoder():
    if tiktoken is None:
        return None
    try:
        return tiktoken.get_encoding("cl100k_base")
    except Exception:
        try:
            return tiktoken.get_encoding("gpt2")
        except Exception:
            return None


def _count_tokens(text: str, encoder) -> int:
    if not text:
        return 0
    if encoder:
        try:
            return len(encoder.encode(text))
        except Exception:
            pass
    return len(text.split())


def _split_by_token_limit(text: str, encoder, max_tokens: int = 15000) -> List[str]:
    if not text:
        return []
    if _count_tokens(text, encoder) <= max_tokens:
        return [text.strip()]

    print("‚ö†Ô∏è  Over token limit detected ‚Äì splitting content further.")

    parts = re.split(r'\n\s*\n', text)
    out = []
    cur = ''

    for p in parts:
        p = p.strip()
        if not p:
            continue

        if not cur:
            if _count_tokens(p, encoder) <= max_tokens:
                cur = p
            else:
                lines = [ln for ln in p.splitlines() if ln.strip()]
                cur2 = ''
                for ln in lines:
                    cand = (cur2 + '\n' + ln) if cur2 else ln
                    if _count_tokens(cand, encoder) <= max_tokens:
                        cur2 = cand
                    else:
                        if cur2:
                            out.append(cur2.strip())
                        cur2 = ln
                if cur2:
                    out.append(cur2.strip())
                cur = ''
        else:
            cand = cur + '\n\n' + p
            if _count_tokens(cand, encoder) <= max_tokens:
                cur = cand
            else:
                out.append(cur.strip())
                if _count_tokens(p, encoder) <= max_tokens:
                    cur = p
                else:
                    lines = [ln for ln in p.splitlines() if ln.strip()]
                    cur2 = ''
                    for ln in lines:
                        cand2 = (cur2 + '\n' + ln) if cur2 else ln
                        if _count_tokens(cand2, encoder) <= max_tokens:
                            cur2 = cand2
                        else:
                            if cur2:
                                out.append(cur2.strip())
                            cur2 = ln
                    if cur2:
                        out.append(cur2.strip())
                    cur = ''

    if cur:
        out.append(cur.strip())
    return out


def fix_vb_het_hieu_luc_formatting(content: str) -> str:
    """Th√™m d·∫•u ch·∫•m sau c·ª•m '(VB h·∫øt hi·ªáu l·ª±c: ...)' n·∫øu ch∆∞a c√≥"""
    pattern = r'\(VB h·∫øt hi·ªáu l·ª±c:\s*\d{1,2}/\d{1,2}/\d{4}\)(?!\.)'
    return re.sub(pattern, r'\g<0>.', content)


def format_file(src_path: Path, out_dir: Path) -> Tuple[Path, int, bool]:
    """Returns (master_path, total_subchunks, has_over_token)"""
    text = src_path.read_text(encoding='utf-8')
    title = extract_title(text, src_path.name)
    chunks = split_into_chunks(text)
    title_pattern = re.escape(title.strip())

    cleaned_chunks = []
    for chunk in chunks:
        c = chunk.strip()
        if not c:
            continue
        c_cleaned = clean_chunk_lines(c, title_pattern).strip()
        if c_cleaned:
            c_cleaned = normalize_newlines(c_cleaned)
            cleaned_chunks.append(c_cleaned)

    chunks = cleaned_chunks
    out_dir.mkdir(parents=True, exist_ok=True)
    master_name = src_path.stem + '.txt'
    master_path = out_dir / master_name

    encoder = _get_token_encoder()
    total_subchunks = 0
    has_over_token = False

    with master_path.open('w', encoding='utf-8') as mf:
        for chunk in chunks:
            subchunks = _split_by_token_limit(chunk, encoder, max_tokens=15000)
            if len(subchunks) > 1:
                has_over_token = True

            merged = []
            for sc in subchunks:
                s = sc.strip()
                if not s:
                    continue
                first_line = s.splitlines()[0].strip() if s.splitlines() else ''
                is_table = first_line.startswith('|')
                is_dieu = re.match(r'^\s*ƒêi·ªÅu\s+\d+\w*\b', first_line, re.I) is not None

                if (is_table or is_dieu) and merged:
                    merged[-1] = merged[-1].rstrip() + '\n' + s
                else:
                    merged.append(s)

            for sc in merged:
                sc_clean = re.sub(r'\n\s*\n\s*(\|)', r'\n\1', sc)
                sc_clean = fix_vb_het_hieu_luc_formatting(sc_clean)
                sc_clean = normalize_newlines(sc_clean.strip())
                mf.write(title.rstrip('.') + '. ' + sc_clean + '\n\n')
                total_subchunks += 1

    return master_path, total_subchunks, has_over_token


def main():
    parser = argparse.ArgumentParser(description='Format crawled files with proper Ch∆∞∆°ng/M·ª•c/ƒêi·ªÅu merging and newline normalization')
    parser.add_argument('--input-dir', default='crawl/dat_dai', help='Input directory with crawled .txt files')
    parser.add_argument('--output-dir', default='format/dat_dai_fix', help='Output directory for formatted files')
    args = parser.parse_args()

    input_dir = Path(args.input_dir)
    output_dir = Path(args.output_dir)

    if not input_dir.exists() or not input_dir.is_dir():
        print(f"Input dir not found: {input_dir}")
        return

    txt_files = sorted([p for p in input_dir.glob('*.txt') if p.name.lower() != 'failed_urls.txt'])
    if not txt_files:
        print(f"No .txt files found in {input_dir}")
        return

    success_count = 0
    failure_count = 0

    for p in txt_files:
        try:
            master_path, nchunks, has_over_token = format_file(p, output_dir)
            if has_over_token:
                print(f"‚ö†Ô∏è  Formatted with over-token split {p.name}: {nchunks} chunks -> {master_path}")
                failure_count += 1
            else:
                print(f"‚úÖ Formatted {p.name}: {nchunks} chunks -> {master_path}")
                success_count += 1
        except Exception as e:
            print(f"‚ùå Error formatting {p.name}: {e}")
            failure_count += 1

    print('\n' + '='*50)
    print(f"‚úÖ Total successful (no over-token): {success_count}")
    print(f"‚ö†Ô∏è  Total with over-token or error: {failure_count}")
    print(f"üìÅ Formatted files written to: {output_dir}")


if __name__ == '__main__':
    main()