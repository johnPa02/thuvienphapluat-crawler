#!/usr/bin/env python3
"""
Multi-threaded batch crawler for thuvienphapluat.vn
Processes multiple URLs concurrently with queue management and progress tracking

Usage:
    python batch_crawler.py urls.txt [--threads 4] [--cookies FILE] [--delay 1] [--retry 3] [--resume]

Example:
    python batch_crawler.py urls.txt --threads 8 --cookies cookies.txt
    python batch_crawler.py urls.txt --resume  # Continue from where we left off
"""

import argparse
import concurrent.futures
import json
import os
import queue
import random
import re
import subprocess
import sys
import threading
import time
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Tuple, Dict


class BatchCrawler:
    """Multi-threaded batch crawler that calls pipeline.py via subprocess"""

    def __init__(self, max_workers: int = 4, cookie_file: str = "cookies.txt",
                 delay_range: Tuple[float, float] = (1.0, 3.0), max_retries: int = 3,
                 output_dir: str = "crawl"):
        self.max_workers = max_workers
        self.cookie_file = cookie_file
        self.delay_range = delay_range
        self.max_retries = max_retries
        self.output_dir = output_dir

        # Thread safety - initialize locks first
        self.stats_lock = threading.Lock()
        self.print_lock = threading.Lock()

        # Create output directory if it doesn't exist
        os.makedirs(self.output_dir, exist_ok=True)

        with self.print_lock:
            print(f"ğŸ“ Output directory: {self.output_dir}")

        # Thread-safe data structures
        self.url_queue = queue.Queue()
        self.completed_queue = queue.Queue()
        self.failed_queue = queue.Queue()

        # Statistics
        self.stats = {
            'total': 0,
            'completed': 0,
            'failed': 0,
            'skipped': 0,
            'start_time': None,
            'end_time': None
        }

        # Resume functionality
        self.completed_urls = set()
        self.failed_urls = set()

        # # Pipeline command
        self.pipeline_cmd = ["uv", "run", "python", "pipeline.py"]

    def extract_doc_name_from_url(self, url: str) -> str:
        """Extract document name from URL"""
        patterns = [
            # VÄƒn báº£n há»£p nháº¥t
            r'Van-ban-hop-nhat-(\d+)-VBHN-VPQH-(\d+)',
            # Nghá»‹ Ä‘á»‹nh
            r'Nghi-dinh-(\d+)-(\d+)-ND-CP',
            # Luáº­t
            r'Luat-(\d+)-(\d+)-QH(\d+)',
            # ThÃ´ng tÆ°
            r'Thong-tu-(\d+)-(\d+)-TT-([A-Z]+)',
            # Quyáº¿t Ä‘á»‹nh
            r'Quyet-dinh-(\d+)-(\d+)-QD-([A-Z]+)',
            # Nghá»‹ quyáº¿t
            r'Nghi-quyet-(\d+)-(\d+)-NQ-([A-Z]+)',
        ]

        for pattern in patterns:
            match = re.search(pattern, url, re.IGNORECASE)
            if match:
                if 'Van-ban-hop-nhat' in url:
                    return f"VÄƒn báº£n há»£p nháº¥t {match.group(1)}/VBHN-VPQH/{match.group(2)}"
                elif 'Nghi-dinh' in url:
                    return f"Nghá»‹ Ä‘á»‹nh {match.group(1)}/{match.group(2)}/NÄ-CP"
                elif 'Luat' in url:
                    return f"Luáº­t {match.group(1)}/{match.group(2)}/QH{match.group(3)}"
                elif 'Thong-tu' in url:
                    return f"ThÃ´ng tÆ° {match.group(1)}/{match.group(2)}/TT-{match.group(3)}"
                elif 'Quyet-dinh' in url:
                    return f"Quyáº¿t Ä‘á»‹nh {match.group(1)}/{match.group(2)}/QÄ-{match.group(3)}"
                elif 'Nghi-quyet' in url:
                    return f"Nghá»‹ quyáº¿t {match.group(1)}/{match.group(2)}/NQ-{match.group(3)}"

        # Handle VÄƒn báº£n sá»­a Ä‘á»•i, bá»• sung, liÃªn quan
        if 'sua-doi' in url:
            if 'Luat' in url:
                # Extract the base law name
                if 'Luat-Doanh-nghiep' in url:
                    return "Luáº­t Doanh nghiá»‡p sá»­a Ä‘á»•i 2025 sá»‘ 76/2025/QH15"
                elif 'Luat-Dau-tu' in url:
                    return "Luáº­t Äáº§u tÆ° sá»­a Ä‘á»•i"
                elif 'Luat-ngan-sach-nha-nuoc' in url:
                    return "Luáº­t NgÃ¢n sÃ¡ch nhÃ  nÆ°á»›c 2025 sá»‘ 89/2025/QH15"
                elif 'Luat-sua-doi-Luat-Dau-tu-cong' in url:
                    return "Luáº­t sá»­a Ä‘á»•i Luáº­t Äáº§u tÆ° cÃ´ng, Luáº­t Äáº§u tÆ° theo phÆ°Æ¡ng thá»©c Ä‘á»‘i tÃ¡c cÃ´ng tÆ°"
                else:
                    # Fallback: try to extract from URL
                    base_match = re.search(r'Luat-([^/]+)', url, re.IGNORECASE)
                    if base_match:
                        base_name = base_match.group(1).replace('-', ' ')
                        return f"Luáº­t {base_name} sá»­a Ä‘á»•i"
                    return "Luáº­t sá»­a Ä‘á»•i"
            elif 'Nghi-dinh' in url:
                base_match = re.search(r'Nghi-dinh-(\d+)-(\d+)-ND-CP', url, re.IGNORECASE)
                if base_match:
                    return f"Nghá»‹ Ä‘á»‹nh {base_match.group(1)}/{base_match.group(2)}/NÄ-CP sá»­a Ä‘á»•i"
                return "Nghá»‹ Ä‘á»‹nh sá»­a Ä‘á»•i"
            elif 'Thong-tu' in url:
                base_match = re.search(r'Thong-tu-(\d+)-(\d+)-TT-([A-Z]+)', url, re.IGNORECASE)
                if base_match:
                    return f"ThÃ´ng tÆ° {base_match.group(1)}/{base_match.group(2)}/TT-{base_match.group(3)} sá»­a Ä‘á»•i"
                return "ThÃ´ng tÆ° sá»­a Ä‘á»•i"
            else:
                # Fallback for other document types
                return "VÄƒn báº£n sá»­a Ä‘á»•i"

        # Handle VÄƒn báº£n liÃªn quan
        elif 'lien-quan' in url or 'cong-van' in url:
            if 'cong-van' in url:
                cong_van_match = re.search(r'cong-van-(\d+)-([A-Z]+)-([A-Z]+)-(\d+)', url, re.IGNORECASE)
                if cong_van_match:
                    doc_type = ""
                    if cong_van_match.group(2) == 'VPCP' and cong_van_match.group(3) == 'DMDN':
                        doc_type = "VÄƒn báº£n liÃªn quan Ä‘áº¿n quáº£n lÃ½ doanh nghiá»‡p"
                    elif cong_van_match.group(2) == 'BKHDT' and cong_van_match.group(3) == 'QLKTTW':
                        doc_type = "VÄƒn báº£n liÃªn quan Ä‘áº¿n quáº£n lÃ½ káº¿ toÃ¡n"

                    return f"{doc_type} sá»‘ {cong_van_match.group(1)}/{cong_van_match.group(4)}"
                return "CÃ´ng vÄƒn liÃªn quan"
            else:
                return "VÄƒn báº£n liÃªn quan"

        # Enhanced fallback: extract meaningful info from URL path
        url_parts = url.split('/')
        if len(url_parts) >= 2:
            # Get the last part (filename)
            filename = url_parts[-1]
            # Remove the .aspx extension and split
            clean_name = filename.replace('.aspx', '')

            # Try to extract document type and content
            if '-Luat-' in clean_name:
                parts = clean_name.split('-Luat-')
                if len(parts) >= 2:
                    doc_type = "Luáº­t"
                    content = parts[1].split('-')[0:3]  # Take first few words
                    content = ' '.join(content)
                    return f"{doc_type} {content}"
            elif '-Nghi-dinh-' in clean_name:
                parts = clean_name.split('-Nghi-dinh-')
                if len(parts) >= 2:
                    doc_type = "Nghá»‹ Ä‘á»‹nh"
                    content = parts[1].split('-')[0:3]
                    content = ' '.join(content)
                    return f"{doc_type} {content}"
            else:
                # Generic fallback - take first few meaningful words
                words = clean_name.split('-')
                if len(words) >= 3:
                    return f"VÄƒn báº£n {' '.join(words[0:3])}"
                elif len(words) >= 2:
                    return f"VÄƒn báº£n {' '.join(words[:2])}"

        return "VÄƒn báº£n"

    def run_pipeline_subprocess(self, url: str, doc_name: str, retry_count: int = 0) -> Tuple[bool, str]:
        """Run pipeline.py as subprocess for a single URL"""
        try:
            with self.print_lock:
                print(f"ğŸš€ [{threading.current_thread().name}] Äang crawl: {doc_name}")

            # Build command - pipeline.py runs from original directory
            # but we tell it where to save files using --output parameter if pipeline supports it
            # or we handle file movement afterwards

            # Try to get absolute path to pipeline.py
            pipeline_path = os.path.join(os.getcwd(), "pipeline.py")
            cmd = ["uv", "run", "python", pipeline_path]
            cmd.append(url)
            cmd.append("--cookies")
            cmd.append(self.cookie_file)

            if doc_name and doc_name != "VÄƒn báº£n":
                cmd.append("--doc-name")
                cmd.append(doc_name)

            # Run subprocess from original directory
            result = subprocess.run(
                cmd,
                cwd=os.getcwd(),
                capture_output=True,
                text=True,
                encoding='utf-8',          # ğŸ‘ˆ ThÃªm dÃ²ng nÃ y
                errors='replace',          # ğŸ‘ˆ Thay kÃ½ tá»± lá»—i báº±ng 
                timeout=300
            )

            if result.returncode == 0:
                # Extract filename from pipeline output
                output_lines = result.stdout.strip().split('\n')
                filename = None

                for line in output_lines:
                    if "ÄÃ£ lÆ°u vÃ o:" in line:
                        # Extract filename from line like "   âœ“ ÄÃ£ lÆ°u vÃ o: Nghá»‹_Ä‘á»‹nh_47-2021-NÄ-CP.txt"
                        filename = line.split("ÄÃ£ lÆ°u vÃ o:")[-1].strip()
                        break

                if filename:
                    # Move file to output directory
                    source_path = os.path.join(os.getcwd(), filename)
                    dest_path = os.path.join(self.output_dir, filename)

                    try:
                        import shutil
                        if os.path.exists(source_path):
                            shutil.move(source_path, dest_path)
                            with self.print_lock:
                                print(f"   [{threading.current_thread().name}] âœ… ÄÃ£ lÆ°u: {filename}")
                            print(f"   [{threading.current_thread().name}] ğŸ“ ÄÃ£ chuyá»ƒn Ä‘áº¿n: {self.output_dir}")
                            return True, filename
                        else:
                            # File might already be in output directory
                            if os.path.exists(dest_path):
                                with self.print_lock:
                                    print(f"   [{threading.current_thread().name}] âœ… ÄÃ£ lÆ°u: {filename}")
                                return True, filename
                            else:
                                with self.print_lock:
                                    print(f"   [{threading.current_thread().name}] âš ï¸  KhÃ´ng tÃ¬m tháº¥y file output")
                                return False, "KhÃ´ng tÃ¬m tháº¥y file output"
                    except Exception as move_error:
                        with self.print_lock:
                            print(f"   [{threading.current_thread().name}] âŒ Lá»—i di chuyá»ƒn file: {move_error}")
                        return False, str(move_error)

                else:
                    # Generate expected filename if not found in output
                    expected_filename = f"{doc_name.replace(' ', '_').replace('/', '_')}.txt"

                    # Check if file exists in current directory or output directory
                    source_path = os.path.join(os.getcwd(), expected_filename)
                    dest_path = os.path.join(self.output_dir, expected_filename)

                    if os.path.exists(source_path):
                        try:
                            import shutil
                            shutil.move(source_path, dest_path)
                            with self.print_lock:
                                print(f"   [{threading.current_thread().name}] âœ… ÄÃ£ lÆ°u: {expected_filename}")
                                print(f"   [{threading.current_thread().name}] ğŸ“ ÄÃ£ chuyá»ƒn Ä‘áº¿n: {self.output_dir}")
                            return True, expected_filename
                        except Exception as move_error:
                            with self.print_lock:
                                print(f"   [{threading.current_thread().name}] âŒ Lá»—i di chuyá»ƒn file: {move_error}")
                            return False, str(move_error)
                    elif os.path.exists(dest_path):
                        with self.print_lock:
                            print(f"   [{threading.current_thread().name}] âœ… ÄÃ£ lÆ°u: {expected_filename}")
                        return True, expected_filename
                    else:
                        with self.print_lock:
                            print(f"   [{threading.current_thread().name}] âš ï¸  Pipeline thÃ nh cÃ´ng nhÆ°ng khÃ´ng tÃ¬m tháº¥y file output")
                        return False, "KhÃ´ng tÃ¬m tháº¥y file output"
            else:
                error_msg = result.stderr.strip() if result.stderr else "Pipeline tháº¥t báº¡i"
                with self.print_lock:
                    print(f"   [{threading.current_thread().name}] âŒ Pipeline lá»—i: {error_msg}")
                    if result.stdout:
                        print(f"   [{threading.current_thread().name}] stdout: {result.stdout[:200]}...")
                return False, error_msg

        except subprocess.TimeoutExpired:
            error_msg = "Pipeline timeout sau 5 phÃºt"
            with self.print_lock:
                print(f"   [{threading.current_thread().name}] â° {error_msg}")
            return False, error_msg

        except Exception as e:
            if retry_count < self.max_retries:
                delay = (retry_count + 1) * 2  # Exponential backoff
                with self.print_lock:
                    print(f"   [{threading.current_thread().name}] âš ï¸  Lá»—i: {e}")
                    print(f"   [{threading.current_thread().name}] ğŸ”„ Thá»­ láº¡i sau {delay}s... (láº§n {retry_count + 1}/{self.max_retries})")

                time.sleep(delay)
                return self.run_pipeline_subprocess(url, doc_name, retry_count + 1)
            else:
                error_msg = f"Tháº¥t báº¡i sau {self.max_retries} láº§n thá»­: {str(e)}"
                with self.print_lock:
                    print(f"   [{threading.current_thread().name}] âŒ {error_msg}")
                return False, error_msg

    def worker_thread(self, thread_id: int):
        """Worker thread function"""
        thread_name = f"Worker-{thread_id:02d}"
        threading.current_thread().name = thread_name

        while True:
            url_item = None
            processed = False
            skipped = False

            try:
                # Get URL from queue
                url_item = self.url_queue.get(timeout=1)

                if url_item is None:  # Poison pill
                    break

                url, doc_name = url_item

                # Check if already completed
                if url in self.completed_urls:
                    with self.print_lock:
                        print(f"   [{thread_name}] â­ï¸  Bá» qua (Ä‘Ã£ hoÃ n thÃ nh): {doc_name}")
                    with self.stats_lock:
                        self.stats['skipped'] += 1
                    skipped = True
                else:
                    # Run pipeline via subprocess
                    success, result = self.run_pipeline_subprocess(url, doc_name)

                    if success:
                        with self.stats_lock:
                            self.stats['completed'] += 1
                            self.completed_urls.add(url)
                        self.completed_queue.put((url, doc_name, result))
                    else:
                        with self.stats_lock:
                            self.stats['failed'] += 1
                            self.failed_urls.add(url)
                        self.failed_queue.put((url, doc_name, result))

                    processed = True

            except queue.Empty:
                continue
            except Exception as e:
                with self.print_lock:
                    print(f"   [{thread_name}] ğŸš¨ Lá»—i worker: {e}")
                with self.stats_lock:
                    self.stats['failed'] += 1
                processed = True  # ÄÃ¡nh dáº¥u Ä‘Ã£ xá»­ lÃ½ lá»—i

            finally:
                if url_item is not None:
                    self.url_queue.task_done()

                    # ğŸ‘‡ ÃP Dá»¤NG DELAY SAU Má»ŒI Xá»¬ LÃ (ká»ƒ cáº£ skip, lá»—i, thÃ nh cÃ´ng)
                    delay = random.uniform(*self.delay_range)
                    time.sleep(delay)
    def load_resume_state(self, resume_file: str = "crawl_state.json"):
        """Load resume state from file"""
        if os.path.exists(resume_file):
            try:
                with open(resume_file, 'r', encoding='utf-8') as f:
                    state = json.load(f)
                    self.completed_urls = set(state.get('completed_urls', []))
                    self.failed_urls = set(state.get('failed_urls', []))

                    print(f"ğŸ“‚ ÄÃ£ táº£i state: {len(self.completed_urls)} hoÃ n thÃ nh, {len(self.failed_urls)} tháº¥t báº¡i")
                    return True
            except Exception as e:
                print(f"âš ï¸  KhÃ´ng thá»ƒ táº£i state file: {e}")
        return False

    def save_resume_state(self, resume_file: str = "crawl_state.json"):
        """Save current state to file"""
        state = {
            'completed_urls': list(self.completed_urls),
            'failed_urls': list(self.failed_urls),
            'timestamp': datetime.now().isoformat(),
            'stats': self.stats,
            'output_dir': self.output_dir
        }

        try:
            with open(resume_file, 'w', encoding='utf-8') as f:
                json.dump(state, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸  KhÃ´ng thá»ƒ lÆ°u state file: {e}")

    def load_urls_from_file(self, url_file: str) -> List[Tuple[str, str]]:
        """Load URLs from file"""
        urls = []
        try:
            with open(url_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    raw = line.rstrip('\n')
                    line = raw.strip()
                    if not line or line.startswith('#'):
                        continue

                    # Support lines with optional doc-name after the URL
                    # e.g. "<url> Luáº­t ngÃ¢n sÃ¡ch 2025"
                    parts = line.split()
                    if parts and parts[0].startswith('http'):
                        url = parts[0]

                        # If there's additional text after the URL, treat it as the doc_name
                        doc_name = None
                        if len(parts) > 1:
                            # Preserve the original spacing for the doc_name portion
                            first_space = raw.find(' ')
                            if first_space != -1:
                                doc_name = raw[first_space + 1 :].strip()

                        # Fallback to extractor when doc_name not provided
                        if not doc_name:
                            doc_name = self.extract_doc_name_from_url(url)

                        urls.append((url, doc_name))
                    else:
                        print(f"âš ï¸  DÃ²ng {line_num}: URL khÃ´ng há»£p lá»‡ - {line}")

            print(f"ğŸ“‹ ÄÃ£ táº£i {len(urls)} URL tá»« {url_file}")
            return urls

        except FileNotFoundError:
            print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file: {url_file}")
            return []
        except Exception as e:
            print(f"âŒ Lá»—i Ä‘á»c file: {e}")
            return []

    def print_progress(self):
        """Print progress information"""
        while True:
            try:
                with self.stats_lock:
                    total = self.stats['total']
                    completed = self.stats['completed']
                    failed = self.stats['failed']
                    skipped = self.stats['skipped']

                    if total > 0:
                        progress = (completed + failed + skipped) / total * 100
                        print(f"\rğŸ“Š Tiáº¿n Ä‘á»™: {progress:.1f}% ({completed + failed + skipped}/{total}) "
                              f"âœ… {completed} âŒ {failed} â­ï¸ {skipped}", end='', flush=True)

                time.sleep(2)

            except KeyboardInterrupt:
                break
            except Exception:
                continue

    def run(self, url_file: str, resume: bool = False):
        """Run the batch crawler"""
        print("ğŸš€ THUVIENPHAPLUAT BATCH CRAWLER")
        print("=" * 60)

        # Load URLs
        urls = self.load_urls_from_file(url_file)
        if not urls:
            return

        # Load resume state if requested
        if resume:
            self.load_resume_state()

        # Filter out already completed URLs
        new_urls = [(url, doc_name) for url, doc_name in urls
                   if url not in self.completed_urls]

        print(f"ğŸ“ Cáº§n crawl: {len(new_urls)} URL")
        print(f"â­ï¸  Bá» qua: {len(urls) - len(new_urls)} URL (Ä‘Ã£ hoÃ n thÃ nh)")

        if not new_urls:
            print("âœ… Táº¥t cáº£ URL Ä‘Ã£ Ä‘Æ°á»£c crawl!")
            return

        # Update statistics
        with self.stats_lock:
            self.stats['total'] = len(urls)
            self.stats['start_time'] = time.time()

        # Add URLs to queue
        for url_item in new_urls:
            self.url_queue.put(url_item)

        print(f"ğŸ”§ Báº¯t Ä‘áº§u crawl vá»›i {self.max_workers} threads...")
        print(f"â±ï¸  Delay: {self.delay_range[0]}-{self.delay_range[1]}s")
        print(f"ğŸ”„ Retry: {self.max_retries} láº§n")
        print()

        # Start progress printer thread
        progress_thread = threading.Thread(target=self.print_progress, daemon=True)
        progress_thread.start()

        # Start worker threads
        workers = []
        for i in range(self.max_workers):
            worker = threading.Thread(target=self.worker_thread, args=(i + 1,))
            worker.start()
            workers.append(worker)

        try:
            # Wait for all URLs to be processed
            self.url_queue.join()

            # Send poison pills to workers
            for _ in workers:
                self.url_queue.put(None)

            # Wait for workers to finish
            for worker in workers:
                worker.join()

        except KeyboardInterrupt:
            print("\n\nâš ï¸  Nháº­n Ctrl+C, Ä‘ang dá»«ng...")

            # Send poison pills
            for _ in workers:
                self.url_queue.put(None)

            # Wait for workers to finish
            for worker in workers:
                worker.join()

            print("âœ… ÄÃ£ dá»«ng an toÃ n")

        # Final statistics
        with self.stats_lock:
            self.stats['end_time'] = time.time()
            duration = self.stats['end_time'] - self.stats['start_time']

            print("\n" + "=" * 60)
            print("ğŸ“Š THá»NG KÃŠ CUá»I CÃ™NG")
            print("=" * 60)
            print(f"Tá»•ng sá»‘ URL: {self.stats['total']}")
            print(f"âœ… HoÃ n thÃ nh: {self.stats['completed']}")
            print(f"âŒ Tháº¥t báº¡i: {self.stats['failed']}")
            print(f"â­ï¸  Bá» qua: {self.stats['skipped']}")
            print(f"â±ï¸  Thá»i gian: {duration:.1f}s")

            if self.stats['completed'] > 0:
                avg_time = duration / self.stats['completed']
                print(f"ğŸš€ Tá»‘c Ä‘á»™: {avg_time:.1f}s/URL")

        # Save state for resume
        self.save_resume_state()

        # Save failed URLs
        if self.failed_urls:
            failed_file = os.path.join(self.output_dir, "failed_urls.txt")
            with open(failed_file, 'w', encoding='utf-8') as f:
                for url in self.failed_urls:
                    f.write(f"{url}\n")
            print(f"ğŸ’¾ ÄÃ£ lÆ°u URL tháº¥t báº¡i vÃ o: {failed_file}")

        print(f"ğŸ“ Output directory: {self.output_dir}")


def main():
    parser = argparse.ArgumentParser(
        description="Multi-threaded batch crawler for thuvienphapluat.vn",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python batch_crawler.py urls.txt
  python batch_crawler.py urls.txt --threads 8 --cookies cookies.txt
  python batch_crawler.py urls.txt --threads 4 --delay 2 5 --retry 2
  python batch_crawler.py urls.txt --resume
        """
    )

    parser.add_argument("url_file", help="File containing list of URLs (one per line)")
    parser.add_argument("-t", "--threads", type=int, default=4,
                       help="Number of concurrent threads (default: 4)")
    parser.add_argument("-c", "--cookies", default="cookies.txt",
                       help="Cookie file (default: cookies.txt)")
    parser.add_argument("-d", "--delay", nargs=2, type=float, default=[3.0, 5.0],
                   metavar=("MIN", "MAX"), help="Delay range between requests (default: 5.0 10.0)")
    parser.add_argument("-r", "--retry", type=int, default=3,
                       help="Number of retries per URL (default: 3)")
    parser.add_argument("--resume", action="store_true",
                       help="Resume from previous run")
    parser.add_argument("--state", default="crawl_state.json",
                       help="State file for resume functionality (default: crawl_state.json)")
    parser.add_argument("-o", "--output-dir", default="crawl",
                       help="Output directory for crawled files (default: crawl)")

    args = parser.parse_args()

    # Validate arguments
    if not os.path.exists(args.url_file):
        print(f"âŒ File khÃ´ng tá»“n táº¡i: {args.url_file}")
        sys.exit(1)

    if args.threads < 1:
        print("âŒ Sá»‘ threads pháº£i >= 1")
        sys.exit(1)

    if args.retry < 0:
        print("âŒ Sá»‘ láº§n thá»­ láº¡i pháº£i >= 0")
        sys.exit(1)

    # Create and run crawler
    crawler = BatchCrawler(
        max_workers=args.threads,
        cookie_file=args.cookies,
        delay_range=tuple(args.delay),
        max_retries=args.retry,
        output_dir=args.output_dir
    )

    try:
        crawler.run(args.url_file, args.resume)
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ Dá»«ng bá»Ÿi ngÆ°á»i dÃ¹ng")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ Lá»—i: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()