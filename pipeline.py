#!/usr/bin/env python3
"""
Pipeline hoÃ n chá»‰nh Ä‘á»ƒ crawl vÃ  xá»­ lÃ½ vÄƒn báº£n phÃ¡p luáº­t tá»« thuvienphapluat.vn

Sá»­ dá»¥ng:
    python pipeline.py <url> [--output FILE] [--cookies FILE] [--doc-name NAME]

VÃ­ dá»¥:
    python pipeline.py "https://thuvienphapluat.vn/van-ban/Doanh-nghiep/Nghi-dinh-47-2021-ND-CP-huong-dan-Luat-Doanh-nghiep-470561.aspx"
    python pipeline.py "https://thuvienphapluat.vn/van-ban/..." --output "luat_abc.txt" --doc-name "Luáº­t ABC 2024"
"""

import argparse
import os
import re
import sys

from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright


def load_cookies_from_file(cookie_file: str) -> list:
    """
    Load cookies tá»« file Netscape format (cookies.txt).
    
    Args:
        cookie_file: ÄÆ°á»ng dáº«n Ä‘áº¿n file cookies.txt
        
    Returns:
        List cÃ¡c cookie dict cho Playwright
    """
    cookies = []
    with open(cookie_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            parts = line.split('\t')
            if len(parts) >= 7:
                domain = parts[0]
                if domain.startswith('.'):
                    domain = domain[1:]
                cookie = {
                    'name': parts[5],
                    'value': parts[6],
                    'domain': domain,
                    'path': parts[2],
                    'secure': parts[3].upper() == 'TRUE',
                    'httpOnly': False,
                }
                try:
                    expires = int(parts[4])
                    if expires > 0:
                        cookie['expires'] = expires
                except:
                    pass
                cookies.append(cookie)
    return cookies


def extract_doc_name_from_url(url: str) -> str:
    """
    Tá»± Ä‘á»™ng trÃ­ch xuáº¥t tÃªn vÄƒn báº£n tá»« URL.
    
    Args:
        url: URL cá»§a vÄƒn báº£n
        
    Returns:
        TÃªn vÄƒn báº£n (vÃ­ dá»¥: "Nghá»‹ Ä‘á»‹nh 47/2021/NÄ-CP")
    """
    # Pattern Ä‘á»ƒ tÃ¬m sá»‘ hiá»‡u vÄƒn báº£n trong URL
    patterns = [
        r'Nghi-dinh-(\d+)-(\d+)-ND-CP',
        r'Luat-(\d+)-(\d+)-QH(\d+)',
        r'Thong-tu-(\d+)-(\d+)-TT-([A-Z]+)',
        r'Quyet-dinh-(\d+)-(\d+)-QD-([A-Z]+)',
        r'Nghi-quyet-(\d+)-(\d+)-NQ-([A-Z]+)',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, url, re.IGNORECASE)
        if match:
            if 'Nghi-dinh' in url:
                return f"Nghá»‹ Ä‘á»‹nh {match.group(1)}/{match.group(2)}/NÄ-CP"
            elif 'Luat' in url:
                return f"Luáº­t {match.group(1)}/{match.group(2)}/QH{match.group(3)}"
            elif 'Thong-tu' in url:
                return f"ThÃ´ng tÆ° {match.group(1)}/{match.group(2)}/TT-{match.group(3)}"
            elif 'Quyet-dinh' in url:
                return f"Quyáº¿t Ä‘á»‹nh {match.group(1)}/{match.group(2)}/QÄ-{match.group(3)}"
            elif 'Nghi-quyet' in url:
                return f"Nghá»‹ quyáº¿t {match.group(1)}/{match.group(2)}/NQ-{match.group(3)}"
    
    return "VÄƒn báº£n"


def crawl_html(url: str, cookie_file: str = None) -> str:
    """
    Crawl HTML tá»« URL vá»›i JavaScript rendering.
    
    Args:
        url: URL cá»§a trang web
        cookie_file: ÄÆ°á»ng dáº«n Ä‘áº¿n file cookies.txt (optional)
        
    Returns:
        HTML content
    """
    print(f"ğŸŒ Äang crawl: {url}")
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context()
        
        if cookie_file and os.path.exists(cookie_file):
            cookies = load_cookies_from_file(cookie_file)
            context.add_cookies(cookies)
            print(f"ğŸª ÄÃ£ load {len(cookies)} cookies tá»« {cookie_file}")
        
        page = context.new_page()
        page.goto(url, wait_until="networkidle", timeout=60000)
        page.wait_for_timeout(3000)
        
        html = page.content()
        browser.close()
    
    return html


def extract_hover_content(soup: BeautifulSoup, element) -> str:
    """
    TrÃ­ch xuáº¥t ná»™i dung hover tooltip tá»« element.
    """
    tooltip_class = None
    
    if element.get('atmm'):
        tooltip_class = element.get('atmm').strip('.')
    elif element.get('onmouseover'):
        match = re.search(r"['\"]\.([^'\"]+)['\"]", element.get('onmouseover'))
        if match:
            tooltip_class = match.group(1)
    
    if not tooltip_class:
        return ""
    
    tooltip_div = soup.find('div', class_=tooltip_class)
    if tooltip_div:
        tooltip_text = tooltip_div.get_text(separator=' ', strip=True)
        if tooltip_text and tooltip_text != "Click vÃ o Ä‘á»ƒ xem ná»™i dung":
            return f" [{tooltip_text}]"
    
    return ""


def extract_note_content(soup: BeautifulSoup, element) -> str:
    """
    TrÃ­ch xuáº¥t ná»™i dung tá»« dvNoteDieuKhoan dá»±a vÃ o id cá»§a element.
    VÃ­ dá»¥: id="span-note_khoan_34_4" -> tÃ¬m div id="note_khoan_34_4"
    """
    element_id = element.get('id', '')
    
    # Láº¥y note_id tá»« span-note_xxx -> note_xxx
    if element_id.startswith('span-'):
        note_id = element_id[5:]  # Bá» "span-"
    else:
        return ""
    
    # TÃ¬m div vá»›i id tÆ°Æ¡ng á»©ng trong dvNoteDieuKhoan
    note_div = soup.find('div', id=note_id)
    if note_div:
        note_text = note_div.get_text(separator=' ', strip=True)
        if note_text:
            # TÃ¡ch láº¥y pháº§n giáº£i thÃ­ch (sau |~|)
            parts = note_text.split('|~|')
            if len(parts) >= 2:
                # Pháº§n Ä‘áº§u lÃ  ná»™i dung bá»• sung (khÃ´ng cÃ³ [])
                # Pháº§n sau lÃ  ghi chÃº nguá»“n (cÃ³ [])
                main_content = parts[0].strip()
                source_note = parts[1].strip() if len(parts) > 1 else ""
                if source_note:
                    return f"\n{main_content} [{source_note}]"
                return f"\n{main_content}"
            return f"\n{note_text}"
    
    return ""


def process_element_with_hover(soup: BeautifulSoup, content_div) -> None:
    """
    Xá»­ lÃ½ cÃ¡c element cÃ³ hover vÃ  chÃ¨n ná»™i dung tooltip vÃ o sau text.
    """
    # Xá»­ lÃ½ cÃ¡c element cÃ³ atmm hoáº·c onmouseover vá»›i lqhlTootip
    hover_elements = content_div.find_all(attrs={'atmm': True})
    hover_elements += content_div.find_all(attrs={'onmouseover': re.compile(r'lqhlTootip', re.I)})
    
    seen = set()
    unique_elements = []
    for el in hover_elements:
        if id(el) not in seen:
            seen.add(id(el))
            unique_elements.append(el)
    
    for element in unique_elements:
        hover_content = extract_hover_content(soup, element)
        if hover_content:
            element.append(hover_content)
    
    # Xá»­ lÃ½ cÃ¡c element <huongdan> vá»›i id="span-note_..."
    huongdan_elements = content_div.find_all('huongdan', id=re.compile(r'^span-note_'))
    for element in huongdan_elements:
        note_content = extract_note_content(soup, element)
        if note_content:
            # Thay tháº¿ text "Bá»• sung" báº±ng ná»™i dung Ä‘áº§y Ä‘á»§
            element.string = note_content


def extract_content(html: str) -> str:
    """
    TrÃ­ch xuáº¥t ná»™i dung text tá»« HTML.
    
    Args:
        html: HTML content
        
    Returns:
        Text content Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a
    """
    print("ğŸ“„ Äang trÃ­ch xuáº¥t ná»™i dung...")
    
    soup = BeautifulSoup(html, "html.parser")
    content_div = soup.find("div", class_="content1")
    
    if content_div is None:
        raise ValueError("KhÃ´ng tÃ¬m tháº¥y tháº» <div class='content1'> trÃªn trang")
    
    # Xá»­ lÃ½ hover tooltips
    process_element_with_hover(soup, content_div)
    
    # Xá»­ lÃ½ cÃ¡c tháº» <b> chá»©a "Äiá»u X." Ä‘á»ƒ tÃ¡ch tÃªn Ä‘iá»u vÃ  ná»™i dung
    # 1. Normalize tÃªn Ä‘iá»u (bá» newline trong tháº» <b>)
    # 2. ThÃªm marker sau tháº» <b> Ä‘á»ƒ xuá»‘ng dÃ²ng
    DIEU_MARKER = "<<<DIEU_NEWLINE>>>"
    for b_tag in content_div.find_all('b'):
        text_content = b_tag.get_text()
        if re.match(r'^Äiá»u\s+\d+\.', text_content):
            # Normalize: thay newline báº±ng space trong tÃªn Ä‘iá»u
            normalized_text = ' '.join(text_content.split())
            b_tag.string = normalized_text
            # ThÃªm marker sau tháº» <b> nÃ y
            from bs4 import NavigableString
            b_tag.insert_after(NavigableString(DIEU_MARKER))
    
    # Láº¥y text
    text = content_div.get_text()
    
    # Thay marker báº±ng newline
    text = text.replace(DIEU_MARKER, '\n')
    
    # Chuáº©n hÃ³a dÃ²ng
    lines = text.split('\n')
    result = []
    buffer = ""
    
    new_paragraph_patterns = [
        r'^ChÆ°Æ¡ng\s+[IVXLCDM]+',
        r'^Má»¥c\s+\d+',
        r'^Äiá»u\s+\d+',
        r'^\d+\.\s',
        r'^[a-zÄ‘]\)\s',
        r'^-\s',
        r'^PHá»¤ Lá»¤C',
        r'^NGHá»Š Äá»ŠNH',
        r'^CÄƒn cá»©',
        r'^Theo Ä‘á» nghá»‹',
        r'^NÆ¡i nháº­n:',
        r'^TM\.',
        r'^Cá»˜NG HÃ’A',
        r'^CHÃNH PHá»¦',
        r'^Sá»‘:',
        r'^HÃ  Ná»™i,',
        r'^Biá»ƒu sá»‘',
        r'^Báº¢NG',
        r'^TT$',
        r'^I\.\s',
        r'^II\.\s',
        r'^III\.\s',
        r'^IV\.\s',
        r'^V\.\s',
        r'^VI\.\s',
    ]
    
    # Pattern Ä‘á»ƒ detect buffer káº¿t thÃºc báº±ng tÃªn Äiá»u (cáº§n xuá»‘ng dÃ²ng sau Ä‘Ã³)
    dieu_title_end_pattern = r'Äiá»u\s+\d+\.\s+[^\n]+$'
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        is_new_paragraph = any(re.match(p, line) for p in new_paragraph_patterns)
        
        if is_new_paragraph:
            if buffer:
                result.append(buffer)
            buffer = line
        else:
            if buffer:
                # Kiá»ƒm tra náº¿u buffer lÃ  tÃªn Äiá»u (káº¿t thÃºc báº±ng Äiá»u X. TÃªn Ä‘iá»u)
                # thÃ¬ xuá»‘ng dÃ²ng thay vÃ¬ ná»‘i
                if re.search(dieu_title_end_pattern, buffer):
                    result.append(buffer)
                    buffer = line
                elif re.search(r'[.;:?!]$', buffer):
                    result.append(buffer)
                    buffer = line
                else:
                    buffer = buffer + " " + line
            else:
                buffer = line
    
    if buffer:
        result.append(buffer)
    
    return '\n'.join(result)


def postprocess(content: str, doc_name: str) -> str:
    """
    Postprocess vÄƒn báº£n phÃ¡p luáº­t.
    
    Args:
        content: Ná»™i dung text thÃ´
        doc_name: TÃªn vÄƒn báº£n phÃ¡p luáº­t
        
    Returns:
        Ná»™i dung Ä‘Ã£ Ä‘Æ°á»£c format
    """
    print("âœ¨ Äang postprocess...")
    
    # Bá» dáº¥u cháº¥m Ä‘á»©ng má»™t mÃ¬nh
    content = re.sub(r'\n\.\n', '\n', content)
    
    # Bá» "[Click vÃ o Ä‘á»ƒ xem ná»™i dung]"
    content = content.replace(' [Click vÃ o Ä‘á»ƒ xem ná»™i dung]', '')
    content = content.replace('[Click vÃ o Ä‘á»ƒ xem ná»™i dung]', '')
    
    # TÃ¡ch sá»‘ khoáº£n ra dÃ²ng má»›i khi bá»‹ dÃ­nh vÃ o ]
    content = re.sub(r'\]\s+(\d+\.)\s*\n', r']\n\1\n', content)
    content = re.sub(r'\]\s+(\d+\.)\s+', r']\n\1 ', content)
    
    # ThÃªm dÃ²ng trá»‘ng vÃ  tÃªn vÄƒn báº£n trÆ°á»›c ChÆ°Æ¡ng
    content = re.sub(r'(ChÆ°Æ¡ng\s+[IVXLCDM]+)', rf'\n{doc_name}. \1', content)
    
    # ThÃªm dÃ²ng trá»‘ng vÃ  tÃªn vÄƒn báº£n trÆ°á»›c Má»¥c
    content = re.sub(r'(Má»¥c\s+\d+\.)', rf'\n{doc_name}. \1', content)
    
    # ThÃªm dÃ²ng trá»‘ng vÃ  tÃªn vÄƒn báº£n trÆ°á»›c I. II. III. ...
    content = re.sub(r'\n((?:I|II|III|IV|V|VI|VII|VIII|IX|X)\.\s+[A-Z])', rf'\n\n{doc_name}. \1', content)
    
    # Ná»‘i dáº¥u ngoáº·c kÃ©p Ä‘á»©ng má»™t mÃ¬nh vÃ o dÃ²ng sau (trÆ°á»ng há»£p bá»‹ xuá»‘ng dÃ²ng trong HTML)
    # Há»— trá»£ cáº£ " thÆ°á»ng vÃ  "" Unicode (U+201C vÃ  U+201D)
    content = re.sub(r'[""\u201c\u201d]\s*\n+\s*(Äiá»u)', r'"\1', content)
    
    # ThÃªm xuá»‘ng dÃ²ng vÃ  tÃªn vÄƒn báº£n trÆ°á»›c má»—i Äiá»u
    # Chá»‰ xá»­ lÃ½ "Äiá»u X." khi nÃ³ lÃ  tiÃªu Ä‘á» (theo sau lÃ  tÃªn Ä‘iá»u - Ã­t nháº¥t 2 tá»«)
    # KhÃ´ng xá»­ lÃ½ khi:
    # - "Äiá»u X." á»Ÿ cuá»‘i cÃ¢u hoáº·c Ä‘á»©ng má»™t mÃ¬nh
    # - "Äiá»u X." náº±m trong ngoáº·c kÃ©p (trÃ­ch dáº«n)
    # Pattern: kÃ½ tá»± khÃ´ng pháº£i newline vÃ  khÃ´ng pháº£i dáº¥u ngoáº·c kÃ©p (cáº£ ASCII " vÃ  Unicode "" ) + "Äiá»u X." + space + chá»¯ cÃ¡i + tÃªn Ä‘iá»u
    content = re.sub(r'([^\n""\u201c\u201d])(Äiá»u\s+\d+\.[ \t]+[A-ZÄÃ€Ãáº¢Ãƒáº Ä‚áº®áº°áº²áº´áº¶Ã‚áº¤áº¦áº¨áºªáº¬ÃˆÃ‰áººáº¼áº¸ÃŠáº¾á»€á»‚á»„á»†ÃŒÃá»ˆÄ¨á»ŠÃ’Ã“á»Ã•á»ŒÃ”á»á»’á»”á»–á»˜Æ á»šá»œá»á» á»¢Ã™Ãšá»¦Å¨á»¤Æ¯á»¨á»ªá»¬á»®á»°á»²Ãá»¶á»¸á»´][a-zÄ‘Ã Ã¡áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã¨Ã©áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»©á»«á»­á»¯á»±á»³Ã½á»·á»¹á»µ]+)', rf'\1\n\n{doc_name}. \2', content)
    # ThÃªm doc_name cho Äiá»u Ä‘Ã£ á»Ÿ Ä‘áº§u dÃ²ng (chÆ°a cÃ³ doc_name) vÃ  theo sau lÃ  tÃªn Ä‘iá»u, khÃ´ng báº¯t Ä‘áº§u báº±ng ngoáº·c kÃ©p
    content = re.sub(r'^(Äiá»u\s+\d+\.[ \t]+[A-ZÄÃ€Ãáº¢Ãƒáº Ä‚áº®áº°áº²áº´áº¶Ã‚áº¤áº¦áº¨áºªáº¬ÃˆÃ‰áººáº¼áº¸ÃŠáº¾á»€á»‚á»„á»†ÃŒÃá»ˆÄ¨á»ŠÃ’Ã“á»Ã•á»ŒÃ”á»á»’á»”á»–á»˜Æ á»šá»œá»á» á»¢Ã™Ãšá»¦Å¨á»¤Æ¯á»¨á»ªá»¬á»®á»°á»²Ãá»¶á»¸á»´][a-zÄ‘Ã Ã¡áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã¨Ã©áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»©á»«á»­á»¯á»±á»³Ã½á»·á»¹á»µ]+)', rf'{doc_name}. \1', content, flags=re.MULTILINE)
    # ThÃªm doc_name cho Äiá»u X. náº±m riÃªng má»™t dÃ²ng (tÃªn Ä‘iá»u á»Ÿ dÃ²ng tiáº¿p theo báº¯t Ä‘áº§u báº±ng chá»¯ hoa)
    content = re.sub(r'^(Äiá»u\s+\d+\.)\n([A-ZÄÃ€Ãáº¢Ãƒáº Ä‚áº®áº°áº²áº´áº¶Ã‚áº¤áº¦áº¨áºªáº¬ÃˆÃ‰áººáº¼áº¸ÃŠáº¾á»€á»‚á»„á»†ÃŒÃá»ˆÄ¨á»ŠÃ’Ã“á»Ã•á»ŒÃ”á»á»’á»”á»–á»˜Æ á»šá»œá»á» á»¢Ã™Ãšá»¦Å¨á»¤Æ¯á»¨á»ªá»¬á»®á»°á»²Ãá»¶á»¸á»´])', rf'{doc_name}. \1 \2', content, flags=re.MULTILINE)
    # Loáº¡i bá» doc_name náº¿u dÃ²ng báº¯t Ä‘áº§u báº±ng ngoáº·c kÃ©p + Äiá»u (trÃ­ch dáº«n) - há»— trá»£ cáº£ ASCII " vÃ  Unicode ""
    content = re.sub(r'["\u201c\u201d]' + re.escape(doc_name) + r'\. (Äiá»u)', r'"\1', content)
    # ThÃªm dÃ²ng trá»‘ng trÆ°á»›c cÃ¡c dÃ²ng báº¯t Ä‘áº§u báº±ng doc_name. Äiá»u (Ä‘áº£m báº£o cÃ³ 1 dÃ²ng trá»‘ng)
    content = re.sub(r'\n(' + re.escape(doc_name) + r'\. Äiá»u)', r'\n\n\1', content)
    
    # Loáº¡i bá» dÃ²ng trá»‘ng thá»«a (nhiá»u hÆ¡n 2 newline liÃªn tiáº¿p)
    content = re.sub(r'\n{3,}', r'\n\n', content)
    
    # Loáº¡i bá» dÃ²ng trá»‘ng thá»«a á»Ÿ Ä‘áº§u file
    content = content.lstrip('\n')
    
    return content


def run_pipeline(url: str, cookie_file: str = "cookies.txt", doc_name: str = None) -> str:
    """
    Cháº¡y pipeline hoÃ n chá»‰nh.
    
    Args:
        url: URL cá»§a vÄƒn báº£n phÃ¡p luáº­t
        output_file: File output (optional)
        cookie_file: File cookies (default: cookies.txt)
        doc_name: TÃªn vÄƒn báº£n (auto-detect náº¿u khÃ´ng cung cáº¥p)
        
    Returns:
        Ná»™i dung vÄƒn báº£n Ä‘Ã£ xá»­ lÃ½
    """
    print("=" * 60)
    print("ğŸš€ THUVIENPHAPLUAT CRAWLER PIPELINE")
    print("=" * 60)
    
    # Auto-detect doc name
    if not doc_name:
        doc_name = extract_doc_name_from_url(url)
    print(f"ğŸ“‹ VÄƒn báº£n: {doc_name}")
    
    # Step 1: Crawl HTML
    html = crawl_html(url, cookie_file if os.path.exists(cookie_file) else None)
    print(f"   âœ“ ÄÃ£ táº£i {len(html):,} bytes HTML")
    
    # Step 2: Extract content
    content = extract_content(html)
    print(f"   âœ“ ÄÃ£ trÃ­ch xuáº¥t {len(content):,} kÃ½ tá»±")
    
    # Step 3: Postprocess
    processed = postprocess(content, doc_name)
    print(f"   âœ“ ÄÃ£ postprocess xong")
    
    # Step 4: ThÃªm doc_name vÃ o Ä‘áº§u file
    processed = f"{doc_name}\n{processed}"
    
    # Step 5: Save output
    output_file = f"{doc_name.replace(' ', '_').replace('/','-')}.txt"
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(processed)
    print(f"   âœ“ ÄÃ£ lÆ°u vÃ o: {output_file}")
    
    print("=" * 60)
    print("âœ… HOÃ€N THÃ€NH!")
    print("=" * 60)
    
    return processed


def main():
    parser = argparse.ArgumentParser(
        description="Crawl vÃ  xá»­ lÃ½ vÄƒn báº£n phÃ¡p luáº­t tá»« thuvienphapluat.vn",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
VÃ­ dá»¥:
  python pipeline.py "https://thuvienphapluat.vn/van-ban/Doanh-nghiep/Nghi-dinh-47-2021-ND-CP-huong-dan-Luat-Doanh-nghiep-470561.aspx"
  
  python pipeline.py "https://thuvienphapluat.vn/van-ban/..." --output "output.txt"
  
  python pipeline.py "https://thuvienphapluat.vn/van-ban/..." --doc-name "Luáº­t ABC 2024"
        """
    )
    
    parser.add_argument("url", help="URL cá»§a vÄƒn báº£n phÃ¡p luáº­t trÃªn thuvienphapluat.vn")
    parser.add_argument("-c", "--cookies", default="cookies.txt", help="File cookies (default: cookies.txt)")
    parser.add_argument("-n", "--doc-name", help="TÃªn vÄƒn báº£n (auto-detect náº¿u khÃ´ng cung cáº¥p)")
    
    args = parser.parse_args()
    
    try:
        run_pipeline(
            url=args.url,
            cookie_file=args.cookies,
            doc_name=args.doc_name
        )
    except Exception as e:
        print(f"âŒ Lá»—i: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
